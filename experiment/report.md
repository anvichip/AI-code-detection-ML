# Experiment Introduction
This project was developed in Google Summer of Code 2025 for LINQS: LAB (Autograder) UC OSPO.
The aim of the project was to create a detection system to produce a confidence score for AI-generated code.

## Experiment Setup
## Datasets
This project utilizes three open-source datasets for training and evaluation:
- [Dataset 1](https://github.com/a0ms1n/AI-Code-Detector-for-Competitive-Programming)
- [Dataset 2](https://github.com/zzarif/AI-Detector/tree/main)  
- [Dataset 3](https://github.com/Back3474/AI-Human-Generated-Program-Code-Dataset/blob/main)

During training, files are named according to their dataset of origin. 
In cases where multiple datasets are combined, filenames follow a concatenated naming convention, e.g., python_1_java_2 indicating python subset of dataset 1 and java subset of dataset 2.

## Dataset Format
The project expects datasets in JSON Lines (JSONL) format.
Each entry must include the following keys:

`writer` : Indicates whether the code was written by a human or generated by AI.  
`code` : Contains the corresponding code snippet.

```json
{"writer": "AI", "code": "#include <bits/stdc++.h>\nusing namespace std;\n\nint main() {\n    ios_base::sync_with_stdio(false);\n    cin.tie(nullptr);\n    \n    int t;\n    cin >> t;\n    while (t--) {\n        string s;\n        cin >> s;\n        char min_char = *min_element(s.begin(), s.end());\n        size_t pos = s.find(min_char);\n        string a(1, min_char);\n        string b = s.substr(0, pos) + s.substr(pos + 1);\n        cout << a << \" \" << b << \"\\n\";\n    }\n    \n    return 0;\n}\n"}
{"writer": "Human", "code": "#include <iostream>\n#include <cmath>\n#include <cstdio>\nusing namespace std;\nint getDown(int n)\n{\n int ans=0;\n while(n>1)\n {\n  ans++;\n  n>>=1;\n }\n return ans;\n}\n#define N 300\nint l[N]={1};\nint main()\n{\n int i,j=2,tmp,n,a,b,p,q;\n tmp=2;\n for(i=2;i<N;i+=2)\n {\n  l[i]=j;\n  if(i==tmp)\n  {\n   tmp<<=1;\n   j++;\n  }\n }\n scanf(\"%d%d%d\",&n,&a,&b);\n p=min(a,b);\n q=max(a,b);\n if(p==q)\n {\n  printf(\"0\\n\");\n }\n else\n {\n  p=p&1?p:p-1;\n  q=q&1?q:q-1;\n  int cha=q-p;\n  //cout<<cha<<\" \"<<getDown(n);\n  if(l[cha]==getDown(n))\n  {\n   printf(\"Final!\\n\");\n  }\n  else\n  {\n   printf(\"%d\\n\",l[cha]);\n  }\n }\n return 0;\n} "}
```

## Dataset Preparation
- Data from the open-source datasets mentioned above was extracted for each language using the scripts:
  
  - `source_files_extractor.py`
  - `csv_data_extraction.py`
  - `json_data_extraction.py`
    
- The extracted language specific datasets are available in the [curated dataset directory](curated_datasets).
- These language specific datasets were then merged using `data_scripts/merge_json.py`.
- Finally, a powerset of the merged language datasets was generated by using `powerset_script.py`, resulting in 63 unique dataset combinations.

## Methodology
- Each dataset in the powerset was trained and evaluated against every other dataset in the powerset.
- This resulted in a total of 3,969 training–testing runs.
- Embeddings were extracted by using [CodeBERT](https://github.com/microsoft/CodeBERT).
- The machine learning models were trained and evaluated using `train_script.py`.
- The following machine learning models were employed in the training process:

  - [**Random Forest**](https://en.wikipedia.org/wiki/Random_forest)  
  - [**Support Vector Machine (SVM)**](https://en.wikipedia.org/wiki/Support_vector_machine)
  - [**XGBoost**](https://en.wikipedia.org/wiki/XGBoost) 
  - [**Multi-Layer Perceptron (MLP)**](https://en.wikipedia.org/wiki/Multilayer_perceptron)
  - [**Ensemble Model**](https://en.wikipedia.org/wiki/Ensemble_learning): [Soft Voting Classifier](https://www.geeksforgeeks.org/machine-learning/voting-classifier/) combining the above four models.

- The complete experiment results can be found [here](https://docs.google.com/spreadsheets/d/1otK4V8OKmIkNpBL08ZQCtp5fkIpYQJi7aa6SDMDiWcE/edit?usp=sharing).

## Research Questions
- **RQ1**: Is it better to use a model trained on more than one language, or a model with few data points?
- **RQ2**: Which family of models performs the best?
- **RQ3**: How well do models trained on one language generalize to unseen languages?
- **RQ4**: How does dataset size affect performance?

- RQ1: Do models benefit from being trained on more than one language?
RQ2: Which family of models performs the best?
RQ3: Is it better to use a model trained on more than one language, or a model with few data points?
RQ4: How well do models trained on one language generalize to unseen languages?
RQ5: How does dataset size affect performance?
  
## Results
### RQ1: 
**Steps**:  
- We conducted 2 comparisons to derive an answer to this question.
- We considered `Java` as training language and compared its F1 score when it was trained with a combination of languages.
  
|        Training Languages        | Testing Language | Accuracy |   F1       |
|:--------------------------------:|:----------------:|:--------:|:----------:|
| java                             | java             | 0.90     | **0.90**   |
| java, cpp                        | java             | 0.62     | 0.45       |
| java, cpp, javascript            | java             | 0.64     | 0.50       |
| java, javascript                 | java             | 0.64     | 0.50       |
| python, java                     | java             | 0.90     | 0.90       |
| python, java, cpp                | java             | 0.62     | 0.45       |
| python, java, cpp, javascript    | java             | 0.64     | 0.50       |
| python, java, javascript         | java             | 0.64     | 0.50       |

**Note**: PHP and Go were excluded from the experiment due to lack of sufficient data.

**Analysis:**  
- The F1 score and accuracy both dropped when java was trained on a combination of languages.

On the other hand, when we inspected the testing of the above models on `cpp` language we observed both increases and decreases in performance.

|       Training Languages        | Testing Languages |   F1   | Accuracy |
|:-------------------------------:|:-----------------:|:------:|:--------:|
| java                            | cpp               | 0.69   | 0.65     |
| java, cpp                       | cpp               | 0.92   | 0.92     |
| java, cpp, javascript           | cpp               | 0.13   | 0.50     |
| java, javascript                | cpp               | 0.13   | 0.50     |
| python, java                    | cpp               | 0.69   | 0.65     |
| python, java, cpp               | cpp               | 0.92   | 0.92     |
| python, java, cpp, javascript   | cpp               | 0.13   | 0.50     |
| python, java, javascript        | cpp               | 0.13   | 0.50     |

**Analysis:**  
- The F1 score and accuracy both dropped and increased when java was trained on a combination of languages but tested on `c++`.
- It is also worthy to note how adding javascript in the mixture of languages significantly reduces the performance.

### RQ2: 
**Steps**:
- The dataset already contains a `Model` column, which represents the family of models (e.g., Random Forest, SVM, Random Forest etc.).
- A column called `Train_Size_Type` was created with the formula:
  
  ``` =IF(G2<40,"Tiny",IF(G2<100,"Small",IF(G2<250,"Medium",IF(G2<500,"Large","Large")))) ```  

  According to the train size number, rows were put into classes - `Tiny`, `Small`, `Medium`, `Large`.

**Setup**
- We took rows where training languae is equal to testing language.
- Then for each of the buckets created earlier we took an average of the F1 score obtained for that model.
- We filtered out the average of all the models with dataset `small`.

Results are as follows:
  
|      Model       |   Average F1   |
|:----------------:|:--------------:|
| SVM              | 0.86           |
| Voting Ensemble  | 0.92           |
| XGBoost          | 0.92           |
| Random Forest    | 0.90           | 
| MLP              | 0.86           |
| SVM              | 0.75           |

- Then, we filtered out the average of all the models with dataset `Large`.

|      Model       | Average F1 |
|:----------------:|:----------:|
| MLP              | 0.94       |
| Voting Ensemble  | 0.94       |
| XGBoost          | 0.94       |
| Random Forest    | 0.92       |
| SVM              | 0.86       |

**Analysis**: 
- Voting Ensemble, XGBoost, and Random Forest perform well in both the setups.

### RQ3: 
**Steps**:
- In order to derive an answer to this question, we created 2 tables.
- One table represented the languages trained and tested on themselves.

| Training Languages | Testing Languages |   F1   |
|:------------------:|:-----------------:|:------:|
| cpp                | cpp               | 0.92   |
| java               | java              | 0.90   |
| javascript         | javascript        | 0.67   |
| php                | php               | 0.00   |
| python             | python            | 0.91   |

- The other table we created, showed the performance of languages when tested on varied unseen languages.

| Training Languages | Testing Languages |   F1   | Accuracy |
|:------------------:|:-----------------:|:------:|:--------:|
| java               | java              | 0.90   | 0.90     |
| java               | cpp               | 0.69   | 0.65     |
| java               | python            | 0.71   | 0.62     |

  A few interesting observation to note:
- Seen Language (java → java)
  - Very strong performance (Accuracy = 0.90, F1 = 0.90).
    Confirms excellent specialization when tested on the training language.
- Unseen Languages
  - cpp: Moderate generalization (Acc = 0.65, F1 = 0.69).
  - python: Slightly weaker (Acc = 0.62, F1 = 0.71).

Cross-Language Patterns
Java → JavaScript shows exceptional transfer, suggesting that models trained on Java can easily adapt to JavaScript due to syntactic and conceptual similarity.
Transfer to C++ and Python is weaker but still better than random, meaning the model does retain useful cross-language representations.

### RQ5: How does dataset size affect performance?
Steps:
- We divided training size into buckets of - 40, 100, 500 as Tiny, Medium, Large respectively.
- Then, we calculated model-wise performance in these buckets.
  
|      Model       | Train_Size_Type |  Average F1  |
|:----------------:|:---------------:|:------------:|
| MLP              | Tiny            | 0.536        |
| MLP              | Small           | 0.8633       |
| MLP              | Large           | 0.94         |
| Random Forest    | Tiny            | 0.536        |
| Random Forest    | Small           | 0.9033       |
| Random Forest    | Large           | 0.92         |
| SVM              | Tiny            | 0.496        |
| SVM              | Small           | 0.75         |
| SVM              | Large           | 0.86         |
| Voting Ensemble  | Tiny            | 0.6          |
| Voting Ensemble  | Small           | 0.9267       |
| Voting Ensemble  | Large           | 0.94         |
| XGBoost          | Tiny            | 0.6          |
| XGBoost          | Small           | 0.9267       |
| XGBoost          | Large           | 0.94         |

**Analysis**
- From the table above, it is visible that model performance does increase with the dataset size.
