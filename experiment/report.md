# Experiment Introduction
This project was developed in Google Summer of Code 2025 for LINQS: LAB (Autograder) UC OSPO.
The aim of the project was to create a detection system to produce a confidence score for AI-generated code.

## Experiment Setup
## Datasets
This project utilizes three open-source datasets for training and evaluation:
- [Dataset 1](https://github.com/a0ms1n/AI-Code-Detector-for-Competitive-Programming)
- [Dataset 2](https://github.com/zzarif/AI-Detector/tree/main)  
- [Dataset 3](https://github.com/Back3474/AI-Human-Generated-Program-Code-Dataset/blob/main)

During training, files are named according to their dataset of origin. 
In cases where multiple datasets are combined, filenames follow a concatenated naming convention, e.g., python_1_java_2 indicating python subset of dataset 1 and java subset of dataset 2.

## Dataset Format
The project expects datasets in JSON Lines (JSONL) format.
Each entry must include the following keys:

`writer` : Indicates whether the code was written by a human or generated by AI.  
`code` : Contains the corresponding code snippet.

```json
{"writer": "AI", "code": "#include <bits/stdc++.h>\nusing namespace std;\n\nint main() {\n    ios_base::sync_with_stdio(false);\n    cin.tie(nullptr);\n    \n    int t;\n    cin >> t;\n    while (t--) {\n        string s;\n        cin >> s;\n        char min_char = *min_element(s.begin(), s.end());\n        size_t pos = s.find(min_char);\n        string a(1, min_char);\n        string b = s.substr(0, pos) + s.substr(pos + 1);\n        cout << a << \" \" << b << \"\\n\";\n    }\n    \n    return 0;\n}\n"}
{"writer": "Human", "code": "#include <iostream>\n#include <cmath>\n#include <cstdio>\nusing namespace std;\nint getDown(int n)\n{\n int ans=0;\n while(n>1)\n {\n  ans++;\n  n>>=1;\n }\n return ans;\n}\n#define N 300\nint l[N]={1};\nint main()\n{\n int i,j=2,tmp,n,a,b,p,q;\n tmp=2;\n for(i=2;i<N;i+=2)\n {\n  l[i]=j;\n  if(i==tmp)\n  {\n   tmp<<=1;\n   j++;\n  }\n }\n scanf(\"%d%d%d\",&n,&a,&b);\n p=min(a,b);\n q=max(a,b);\n if(p==q)\n {\n  printf(\"0\\n\");\n }\n else\n {\n  p=p&1?p:p-1;\n  q=q&1?q:q-1;\n  int cha=q-p;\n  //cout<<cha<<\" \"<<getDown(n);\n  if(l[cha]==getDown(n))\n  {\n   printf(\"Final!\\n\");\n  }\n  else\n  {\n   printf(\"%d\\n\",l[cha]);\n  }\n }\n return 0;\n} "}
```

## Dataset Preparation
- Data from the open-source datasets mentioned above was extracted for each language using the scripts:
  
  - `source_files_extractor.py`
  - `csv_data_extraction.py`
  - `json_data_extraction.py`
    
- The extracted language specific datasets are available in the [curated dataset directory](curated_datasets).
- These language specific datasets were then merged using `data_scripts/merge_json.py`.
- Finally, a powerset of the merged language datasets was generated by using `powerset_script.py`, resulting in 63 unique dataset combinations.

## Methodology
- Each dataset in the powerset was trained and evaluated against every other dataset in the powerset.
- This resulted in a total of 3,969 training–testing runs.
- Embeddings were extracted by using [CodeBERT](https://github.com/microsoft/CodeBERT).
- The machine learning models were trained and evaluated using `train_script.py`.
- The following machine learning models were employed in the training process:

  - [**Random Forest**](https://en.wikipedia.org/wiki/Random_forest)  
  - [**Support Vector Machine (SVM)**](https://en.wikipedia.org/wiki/Support_vector_machine)
  - [**XGBoost**](https://en.wikipedia.org/wiki/XGBoost) 
  - [**Multi-Layer Perceptron (MLP)**](https://en.wikipedia.org/wiki/Multilayer_perceptron)
  - [**Ensemble Model**](https://en.wikipedia.org/wiki/Ensemble_learning): [Soft Voting Classifier](https://www.geeksforgeeks.org/machine-learning/voting-classifier/) combining the above four models.

- The complete experiment results can be found [here](https://docs.google.com/spreadsheets/d/1otK4V8OKmIkNpBL08ZQCtp5fkIpYQJi7aa6SDMDiWcE/edit?usp=sharing).

## Research Questions
- **RQ1**: “Is it better to use a model trained on multiple languages, or a model trained on a single language with fewer data points?”
- **RQ2**: Which family of models performs the best?
- **RQ3**: How well do models trained on one language generalize to unseen languages?
- **RQ4**: How does dataset size affect performance?

## Results
### RQ1: Is it better to use a model trained on more than one language, or a model with few data points?
**Steps**:  
We performed two sets of comparisons:
- Training on Java (alone or in combination) and testing on Java.
- Training on Java (alone or in combination) and testing on C++.

**Case 1: Training on Java and testing on Java**
|        Training Languages        | Testing Language | Accuracy |   F1       |
|:--------------------------------:|:----------------:|:--------:|:----------:|
| java                             | java             | 0.90     | **0.90**   |
| java, cpp                        | java             | 0.62     | 0.45       |
| java, cpp, javascript            | java             | 0.64     | 0.50       |
| java, javascript                 | java             | 0.64     | 0.50       |
| python, java                     | java             | 0.90     | 0.90       |
| python, java, cpp                | java             | 0.62     | 0.45       |
| python, java, cpp, javascript    | java             | 0.64     | 0.50       |
| python, java, javascript         | java             | 0.64     | 0.50       |

**Note**: PHP and Go were excluded from the experiment due to lack of sufficient data.

**Analysis**:
- Models trained only on Java performed best when tested on Java.
- Adding C++ or JavaScript generally degraded performance.
- Interestingly, Python + Java preserved Java performance (0.90 F1), suggesting that certain language pairs can complement each other.

**Case 2: Training on Java and testing on C++**
|       Training Languages        | Testing Languages |   F1   | Accuracy |
|:-------------------------------:|:-----------------:|:------:|:--------:|
| java                            | cpp               | 0.69   | 0.65     |
| java, cpp                       | cpp               | 0.92   | 0.92     |
| java, cpp, javascript           | cpp               | 0.13   | 0.50     |
| java, javascript                | cpp               | 0.13   | 0.50     |
| python, java                    | cpp               | 0.69   | 0.65     |
| python, java, cpp               | cpp               | 0.92   | 0.92     |
| python, java, cpp, javascript   | cpp               | 0.13   | 0.50     |
| python, java, javascript        | cpp               | 0.13   | 0.50     |

**Analysis**:
- Adding C++ during training dramatically improved performance when testing on C++ (F1 0.92 vs. 0.69).
- Adding JavaScript caused performance to collapse (F1 0.13), showing that some languages in the mix can be detrimental.
- Python had a neutral effect—it neither helped nor hurt significantly.

**Conclusion**:
- Training on multiple languages is not always beneficial.
- The impact depends on the similarity and compatibility of languages:
  - Java with C++ pairing improved generalization.
  - Java with JavaScript pairing degraded performance severely.

### RQ2: Which family of models performs the best?
**Steps**:
- The dataset already contains a `Model` column, which represents the family of models (e.g., Random Forest, SVM, Random Forest etc.).
- We created a Train_Size_Type column using the formula:
  
  ``` =IF(G2<40,"Tiny",IF(G2<100,"Small",IF(G2<250,"Medium",IF(G2<500,"Large","Large")))) ```  

  This classified training datasets into four groups: `Tiny`, `Small`, `Medium`, and `Large`
  
- For this analysis, we considered only rows where the training and testing languages were the same
- For each model and size group, we computed the average F1 score.

**Results (by training size)**:
| Model               | Tiny | Small | Large |
|---------------------|:----:|:-----:|:-----:|
| **MLP**             | 0.54 | 0.86  | 0.94  |
| **Random Forest**   | 0.54 | 0.90  | 0.92  |
| **SVM**             | 0.50 | 0.75  | 0.86  |
| **Voting Ensemble** | 0.60 | 0.93  | 0.94  |
| **XGBoost**         | 0.60 | 0.93  | 0.94  |

**Results (overall averages)**:
| Model           | Avg F1   |
| :--------------:| :------: |
| Voting Ensemble | **0.94** |
| XGBoost         | **0.94** |
| MLP             | 0.92     |
| Random Forest   | 0.92     |
| SVM             | 0.83     |

**Analysis**:
- **Tiny datasets**: All models perform poorly (<0.60 F1), highlighting the difficulty of learning with very limited data.
- **Scaling with data**:
  - MLP, Voting Ensemble, and XGBoost benefit most from larger datasets, reaching ~0.94 F1.
  - Random Forest improves steadily but slightly lags behind at large scale (0.92).
  - SVM performs the worst overall, though it improves with more data.
- **Consistency**: Voting Ensemble and XGBoost not only achieve the highest averages but also show strong performance across sizes.

**Conclusions**:
- Voting Ensemble and XGBoost are the best-performing model families overall, achieving ~0.94 F1 on average.
- MLP and Random Forest are strong contenders, particularly when enough training data is available.
- SVM lags behind, especially on small datasets.
- Performance is strongly tied to dataset size, with most models requiring at least “Small” training sets to perform competitively.

### RQ3: How well do models trained on one language generalize to unseen languages? 
**Steps**:
- In order to derive an answer to this question, we created 2 tables.
- One table represented the languages trained and tested on themselves.

| Training Languages | Testing Languages |   F1   |
|:------------------:|:-----------------:|:------:|
| cpp                | cpp               | 0.92   |
| java               | java              | 0.90   |
| javascript         | javascript        | 0.67   |
| php                | php               | 0.00   |
| python             | python            | 0.91   |

- The other table we created, showed the performance of languages when tested on varied unseen languages.

| Training Languages | Testing Languages |   F1   | Accuracy |
|:------------------:|:-----------------:|:------:|:--------:|
| java               | java              | 0.90   | 0.90     |
| java               | cpp               | 0.69   | 0.65     |
| java               | python            | 0.71   | 0.62     |

  A few interesting observation to note:
- Seen Language (java → java)
  - Very strong performance (Accuracy = 0.90, F1 = 0.90).
    Confirms excellent specialization when tested on the training language.
- Unseen Languages
  - cpp: Moderate generalization (Acc = 0.65, F1 = 0.69).
  - python: Slightly weaker (Acc = 0.62, F1 = 0.71).

Cross-Language Patterns
Java → JavaScript shows exceptional transfer, suggesting that models trained on Java can easily adapt to JavaScript due to syntactic and conceptual similarity.
Transfer to C++ and Python is weaker but still better than random, meaning the model does retain useful cross-language representations.

### RQ5: How does dataset size affect performance?
Steps:
- We divided training size into buckets of - 40, 100, 500 as Tiny, Medium, Large respectively.
- Then, we calculated model-wise performance in these buckets using `Average F1 Score`.
  
![Graph](graph_1.png)

**Analysis**
- From the graph above, it is visible that model performance does increase with the dataset size but saturated after a point in the dataset. 
