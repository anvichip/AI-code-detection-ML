# AI Code Dectection

## Project Introduction
This project focuses on the use of machine learning techniques to distinguish between human-written and AI-generated code. 
CodeBERT is used to extract code embeddings, which are then used to train classification models.

The following machine learning models were trained and used to classification tasks:
  - [**Random Forest**](https://en.wikipedia.org/wiki/Random_forest)  
  - [**Support Vector Machine (SVM)**](https://en.wikipedia.org/wiki/Support_vector_machine)
  - [**XGBoost**](https://en.wikipedia.org/wiki/XGBoost) 
  - [**Multi-Layer Perceptron (MLP)**](https://en.wikipedia.org/wiki/Multilayer_perceptron)
  - [**Ensemble Model**](https://en.wikipedia.org/wiki/Ensemble_learning): A [soft-voting](https://www.geeksforgeeks.org/machine-learning/voting-classifier/) ensemble that combines Random Forest, SVM, XGBoost, and MLP by averaging their predicted probabilities.

## Installation 
```
git clone https://github.com/anvichip/AI-code-detection-ML.git
cd AI-code-detection-ML
pip install -r requirements.txt
```

## Datasets
This project utilizes three open-source datasets for training and evaluation:
- [Dataset 1](https://github.com/a0ms1n/AI-Code-Detector-for-Competitive-Programming)
- [Dataset 2](https://github.com/zzarif/AI-Detector/tree/main)  
- [Dataset 3](https://github.com/Back3474/AI-Human-Generated-Program-Code-Dataset/blob/main)

During training, files are named according to their dataset of origin. 
In cases where multiple datasets are combined, filenames follow a concatenated naming convention, e.g., python_1_java_2 indicating python subset of dataset 1 and java subset of dataset 2.

## Dataset Format
The project expects datasets in JSON Lines (JSONL) format.
Each entry must include the following keys:

`writer` : Indicates whether the code was written by a human or generated by AI.  
`code` : Contains the corresponding code snippet.

```json
{"writer": "AI", "code": "#include <bits/stdc++.h>\nusing namespace std;\n\nint main() {\n    ios_base::sync_with_stdio(false);\n    cin.tie(nullptr);\n    \n    int t;\n    cin >> t;\n    while (t--) {\n        string s;\n        cin >> s;\n        char min_char = *min_element(s.begin(), s.end());\n        size_t pos = s.find(min_char);\n        string a(1, min_char);\n        string b = s.substr(0, pos) + s.substr(pos + 1);\n        cout << a << \" \" << b << \"\\n\";\n    }\n    \n    return 0;\n}\n"}
{"writer": "Human", "code": "#include <iostream>\n#include <cmath>\n#include <cstdio>\nusing namespace std;\nint getDown(int n)\n{\n int ans=0;\n while(n>1)\n {\n  ans++;\n  n>>=1;\n }\n return ans;\n}\n#define N 300\nint l[N]={1};\nint main()\n{\n int i,j=2,tmp,n,a,b,p,q;\n tmp=2;\n for(i=2;i<N;i+=2)\n {\n  l[i]=j;\n  if(i==tmp)\n  {\n   tmp<<=1;\n   j++;\n  }\n }\n scanf(\"%d%d%d\",&n,&a,&b);\n p=min(a,b);\n q=max(a,b);\n if(p==q)\n {\n  printf(\"0\\n\");\n }\n else\n {\n  p=p&1?p:p-1;\n  q=q&1?q:q-1;\n  int cha=q-p;\n  //cout<<cha<<\" \"<<getDown(n);\n  if(l[cha]==getDown(n))\n  {\n   printf(\"Final!\\n\");\n  }\n  else\n  {\n   printf(\"%d\\n\",l[cha]);\n  }\n }\n return 0;\n} "}
```

## Training
To train the machine learning models on a JSONL dataset, run the following command:
```
python3 -m scripts.train_script --dataset path/to/your/dataset.jsonl --language programming languages being used in training --save_dir results 
```

## Classification
To classify whether a given code snippet is AI-generated or human-written, use the `classify_script.py`:
```
python3 -m scripts.classify_script --tokenizer CodeBERT or TF-IDF --model "" --code_file path/to/your/code/file --trained_models_path path/to/your/trained/models
```

## Experiment
### Dataset Preparation
- Data from the open-source datasets mentioned above was extracted for each language using the scripts:
  
  - `source_files_extractor.py`
  - `csv_data_extraction.py`
  - `json_data_extraction.py`
    
- The extracted language specific datasets are available in the [curated dataset directory](curated_datasets).
- These language specific datasets were then merged using `data_scripts/merge_json.py`.
- Finally, a powerset of the merged language datasets was generated by using `powerset_script.py`, resulting in 63 unique dataset combinations.

### Methodology
- Each dataset in the powerset was trained and evaluated against every other dataset in the powerset.
- This resulted in a total of 3,969 training–testing runs.
- Embeddings were extracted by using [CodeBERT](https://github.com/microsoft/CodeBERT).
- The machine learning models were trained and evaluated using `train_script.py`.
- The following machine learning models were employed in the training process:

  - [**Random Forest**](https://en.wikipedia.org/wiki/Random_forest)  
  - [**Support Vector Machine (SVM)**](https://en.wikipedia.org/wiki/Support_vector_machine)
  - [**XGBoost**](https://en.wikipedia.org/wiki/XGBoost) 
  - [**Multi-Layer Perceptron (MLP)**](https://en.wikipedia.org/wiki/Multilayer_perceptron)
  - [**Ensemble Model**](https://en.wikipedia.org/wiki/Ensemble_learning): [Soft Voting Classifier](https://www.geeksforgeeks.org/machine-learning/voting-classifier/) combining the above four models.

- The complete experiment results can be found [here](https://docs.google.com/spreadsheets/d/1otK4V8OKmIkNpBL08ZQCtp5fkIpYQJi7aa6SDMDiWcE/edit?usp=sharing).

### Research Questions
- **RQ1**: Do models benefit from being trained on more than one language? 
- **RQ2**: Which family of models performs the best?
- **RQ3**: Is it better to use a model trained on more than one language, or a model with few data points?
- **RQ4**: How well do models trained on one language generalize to unseen languages?
- **RQ5**: How does dataset size affect performance?
  
### Results
**RQ1**: 
Steps:  
- A new derived column named `Train_Type` was created to categorize models as being trained on either a Single Language or Multiple Languages.
- Then, a pivot table was inserted using the configuration:
- Rows:
  - `Train_Type`
- Values:
  - `Accuracy (Average)`
  - `Precision (Average)`
  - `Recall (Average)`
  - `F1 (Average)`
  
**Experiment Table:** 
| Train_Type | AVERAGE of Accuracy | AVERAGE of Precision | AVERAGE of Recall | AVERAGE of F1 |
|:----------:|:-------------------:|:--------------------:|:-----------------:|:-------------:|
| Multiple   | 0.6568242829        | 0.565785575          | 0.6589429128      | 0.5612754107  |
| Single     | 0.6350984127        | 0.5765079365         | 0.646984127       | 0.5503619048  |

**Analysis:**  
- Accuracy: Models trained on multiple languages (0.657) slightly outperform single-language ones (0.635).
- Recall: Again, multiple-language models (0.659) are better than single (0.647), suggesting they are more likely to correctly identify positives.
- Precision: Single-language models (0.577) edge out multiple (0.566), meaning they make fewer false positives.
- F1 Score: Multiple (0.561) > Single (0.550), showing a balanced gain overall.

**RQ2**: 
**Steps**:
- The dataset already contains a Model column, which represents the family of models (e.g., Random Forest, SVM, Neural Network, etc.).
- Pivot table was created with the following configuration:
- Rows:
  - `Model`
- Values:
  - `Accuracy (Average)`
  - `Precision (Average)`
  - `Recall (Average)`
  - `F1 (Average)`

**Experiment Table:** 
|      Model       | AVERAGE of Accuracy | AVERAGE of Precision | AVERAGE of Recall | AVERAGE of F1 |
|:----------------:|:-------------------:|:--------------------:|:-----------------:|:-------------:|
| MLP              | 0.6824270353        | 0.6530261137         | 0.5802150538      | 0.5715412186  |
| Random Forest    | 0.6319098822        | 0.5203174603         | 0.5984229391      | 0.5010240655  |
| SVM              | 0.5501075269        | 0.4559344598         | 0.8366205837      | 0.5805325141  |
| Voting Ensemble  | 0.6964669739        | 0.5918689196         | 0.6493804403      | 0.5791295443  |
| XGBoost          | 0.7144495648        | 0.6121044547         | 0.6252534562      | 0.5697491039  |

**Analysis**: 
- XGBoost: Highest accuracy and precision, but moderate recall.
- MLP: Best precision, meaning fewer false positives, with balanced overall performance.
- SVM: Extremely high recall, meaning it rarely misses positives, but its accuracy and precision are low → prone to false alarms.
- Voting Ensemble: Well-rounded across all metrics, second-best accuracy and good recall, making it a balanced choice.

**RQ3**: 
**Steps**:
- A new derived column `Data_Size_Category` was created using the formula:
  ```=IF(G2<50,"Small",IF(G2<500,"Medium","Large"))```
- Pivot table was created with the following configuration:
- Rows:
  - `Train_Type`
- Columns:
  - `Train_Size_Type`
- Values:
  - `Accuracy (Average)`

**Experiment Table:** 
| Train_Type |    Large    |   Medium    |    Small    |
|:----------:|:-----------:|:-----------:|:-----------:|
| Multiple   | 0.7029205021 | 0.6435147392 | 0.641755575  |
| Single     | 0.7308333333 | 0.6842597403 | 0.5518297872 |

**Analysis**: 
- Large Training Sets
  - Single-language models (0.731) slightly outperform multi-language models (0.703).
      - Suggests that when enough data is available, single-language specialization yields the best accuracy.
  - Medium Training Sets
     - Single (0.684) > Multiple (0.644).
    - Still, single-language training holds a small advantage when data is moderately sized.
  - Small Training Sets
     - Multiple (0.642) far outperforms single (0.552).
     - With limited data, having language diversity helps compensate for low sample size.

## Convert to tsv file
Optionally, you can choose to convert your results into a .tsv for easier readability by using `tsv_creator_script.py`
```
python3 -m scripts.tsv_creator_script --data_dir path/to/your/results/directory --metrics_file codebert_metrics.json --output_tsc name_to_save_tsv_file
```
